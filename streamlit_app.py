#!/usr/bin/env python3
"""
Streamlit Web Interface for Blog Extractor Tool
Provides a user-friendly web interface for extracting blog posts and converting to WordPress XML.
"""

import streamlit as st
import time
import asyncio
from typing import List, Dict, Any
from datetime import datetime
from urllib.parse import urlparse
from collections import Counter

# Import our blog extractor
from blog_extractor import BlogExtractor

# Page configuration
st.set_page_config(
    page_title="Blog Extractor Tool",
    page_icon="üì∞",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Hide anchor links from headers
st.markdown("""
<style>
    /* Hide anchor links from headers */
    .stMarkdown h1 a,
    .stMarkdown h2 a,
    .stMarkdown h3 a,
    .stMarkdown h4 a {
        display: none !important;
    }

    /* Better spacing for the app */
    .block-container {
        padding-top: 2rem;
    }
</style>
""", unsafe_allow_html=True)

def setup_session_state():
    """Initialize session state variables"""
    if 'extraction_results' not in st.session_state:
        st.session_state.extraction_results = []
    if 'is_processing' not in st.session_state:
        st.session_state.is_processing = False
    if 'extraction_complete' not in st.session_state:
        st.session_state.extraction_complete = False
    if 'error_log' not in st.session_state:
        st.session_state.error_log = []
    if 'duplicate_log' not in st.session_state:
        st.session_state.duplicate_log = []
    if 'xml_content' not in st.session_state:
        st.session_state.xml_content = None
    if 'links_content' not in st.session_state:
        st.session_state.links_content = None
    if 'link_analysis' not in st.session_state:
        st.session_state.link_analysis = None
    if 'replacements' not in st.session_state:
        st.session_state.replacements = []

def display_header():
    """Display the main header and instructions"""
    st.title("üì∞ Blog Extractor Tool")
    st.markdown("**Convert blog posts to WordPress XML format with comprehensive link extraction**")

    st.info("üí° **Quick Start:** 1) Paste URLs ‚Üí 2) Click the big blue button ‚Üí 3) Download XML")

    with st.expander("üìã How to Use This Tool", expanded=False):
        st.markdown("""
        ### Instructions:
        1. **Paste URLs**: Copy and paste your blog URLs into Step 1 (one per line)
        2. **Click Extract**: Hit the big blue "EXTRACT BLOG POSTS NOW" button
        3. **Download**: Scroll down to get your WordPress XML file

        ### Optional:
        - Expand Step 2 to enable concurrent processing for 3-5x faster extraction

        ### What This Tool Does:
        - ‚úÖ Extracts blog content from any website (Wix, WordPress, Medium, etc.)
        - ‚úÖ Gets titles, content, authors, dates, categories, and tags
        - ‚úÖ Finds all links within blog posts
        - ‚úÖ Creates WordPress XML file for easy import
        - ‚úÖ Works with JavaScript-heavy sites
        - ‚úÖ Supports concurrent processing (3-5x faster!)
        """)

def get_sample_urls() -> List[str]:
    """Return sample URLs for testing"""
    return [
        "https://www.blog.kirkbrotherschevroletofvicksburg.com/2024/09/19/2024-chevy-silverado-hd-trucks/",
        "https://www.blog.kirkbrotherschevroletofvicksburg.com/2024/09/18/2024-chevy-colorado-zr2/",
        "https://www.blog.kirkbrotherschevroletofvicksburg.com/2024/09/17/2024-chevy-trailblazer/",
    ]

def validate_urls(urls: List[str]) -> List[str]:
    """Validate and clean URL list"""
    valid_urls = []
    for url in urls:
        url = url.strip()
        if url and (url.startswith('http://') or url.startswith('https://')):
            valid_urls.append(url)
    return valid_urls

def analyze_links(extraction_results: List[Dict]) -> Dict[str, Any]:
    """Analyze all extracted links and categorize them with anchor text"""
    source_domains = set()
    internal_links = {}  # {url: {'count': N, 'texts': [anchor texts], 'sources': [blog post URLs]}}
    external_links = {}

    # Collect source domains
    for result in extraction_results:
        if 'url' in result:
            source_domains.add(urlparse(result['url']).netloc)

    # Analyze each link
    for result in extraction_results:
        post_url = result.get('url', 'Unknown')
        for link in result.get('links', []):
            url = link['url']
            text = link.get('text', 'No text')

            # Skip anchor-only links (#section)
            if url.startswith('#'):
                continue

            # Categorize as internal or external
            link_domain = urlparse(url).netloc
            is_internal = link_domain in source_domains

            target_dict = internal_links if is_internal else external_links

            if url not in target_dict:
                target_dict[url] = {
                    'count': 0,
                    'texts': set(),
                    'sources': set()
                }

            target_dict[url]['count'] += 1
            target_dict[url]['texts'].add(text)
            target_dict[url]['sources'].add(post_url)

    # Convert sets to lists for serialization
    for link_dict in [internal_links, external_links]:
        for url in link_dict:
            link_dict[url]['texts'] = list(link_dict[url]['texts'])
            link_dict[url]['sources'] = list(link_dict[url]['sources'])

    return {
        'internal': internal_links,
        'external': external_links,
        'source_domains': list(source_domains)
    }

def get_url_inputs():
    """Simple URL input with immediate extract button"""
    # Use markdown without anchor links
    st.markdown("### üìù Step 1: Enter Your Blog URLs")
    st.caption("Paste your blog URLs below (one per line)")

    # Two column layout: text area + button
    col1, col2 = st.columns([3, 1])

    with col1:
        # Text area
        url_text = st.text_area(
            "Blog URLs:",
            height=250,
            placeholder="https://example.com/blog/post1\nhttps://example.com/blog/post2\nhttps://example.com/blog/post3",
            label_visibility="collapsed"
        )

    with col2:
        # Add some vertical spacing to align with text area
        st.write("")
        st.write("")

        # Always show button, but disable if no valid URLs
        urls = []
        if url_text:
            urls = [url.strip() for url in url_text.strip().split('\n') if url.strip()]

        valid_urls = validate_urls(urls)

        # Show button always, disable if no URLs
        button_clicked = st.button(
            "üöÄ **EXTRACT POSTS**",
            type="primary",
            use_container_width=True,
            disabled=len(valid_urls) == 0,
            key="extract_button_top",
            help="Click to start extracting blog posts"
        )

        if valid_urls:
            st.success(f"‚úÖ {len(valid_urls)} ready")
        else:
            st.info("üëÜ Paste URLs first")

        st.caption("*or configure options below*")

    return valid_urls, button_clicked


def get_concurrent_settings() -> int:
    """Get concurrent processing settings"""
    st.markdown("### ‚ö° Step 2 (Optional): Performance Settings")
    st.caption("Skip this section to use default settings, or expand to boost speed")

    with st.expander("‚öôÔ∏è Advanced Performance Options"):
        enable_concurrent = st.checkbox(
            "Enable concurrent processing (3-5x faster!)",
            value=False,
            help="Process multiple URLs simultaneously for major speed boost"
        )

        if enable_concurrent:
            max_concurrent = st.slider(
                "Max concurrent requests:",
                min_value=2,
                max_value=10,
                value=5,
                help="Higher = faster, but may overwhelm the server. 5 is recommended."
            )
            st.info(f"üí® Will process {max_concurrent} URLs simultaneously")
            return max_concurrent
        else:
            st.info("üê¢ Sequential processing (one URL at a time)")
            return 1

    return 1  # Default to sequential

def display_link_analysis():
    """Display unique internal and external links with anchor text"""
    if not st.session_state.get('link_analysis'):
        return

    analysis = st.session_state.link_analysis
    internal = analysis.get('internal', {})
    external = analysis.get('external', {})

    st.markdown("### üîó Link Analysis")

    # Internal Links
    if internal:
        with st.expander(f"üìä Internal Links ({len(internal)} unique)", expanded=True):
            st.markdown("*These links point to the same domain as your blog posts*")

            # Sort by count descending - handle both old and new format
            try:
                sorted_links = sorted(internal.items(), key=lambda x: x[1]['count'] if isinstance(x[1], dict) else x[1], reverse=True)
            except (KeyError, TypeError):
                st.error("‚ö†Ô∏è Link data format mismatch. Please re-run the extraction.")
                return

            for url, data in sorted_links[:30]:  # Show top 30
                # Handle both old format (int) and new format (dict)
                if isinstance(data, dict):
                    with st.container():
                        col1, col2 = st.columns([5, 1])
                        with col1:
                            # Show anchor text(s)
                            anchor_texts = data.get('texts', ['No text'])[:3]
                            st.markdown(f"**{anchor_texts[0]}**" + (f" *(+{len(data['texts'])-1} more)*" if len(data.get('texts', [])) > 1 else ""))
                            st.caption(url if len(url) <= 80 else url[:80] + '...')

                            # Show source blog posts
                            sources = data.get('sources', [])[:3]
                            if sources:
                                source_text = ", ".join([f"[Post]({src})" for src in sources])
                                if len(data.get('sources', [])) > 3:
                                    source_text += f" *+{len(data.get('sources', []))-3} more*"
                                st.markdown(f"Found in: {source_text}", unsafe_allow_html=True)
                        with col2:
                            st.metric("Uses", data.get('count', 0))
                        st.markdown("---")
                else:
                    # Old format: just show count
                    st.text(f"{url[:80]}... (√ó{data})")
                    st.markdown("---")

            if len(sorted_links) > 30:
                st.info(f"... and {len(sorted_links) - 30} more internal links")

    # External Links
    if external:
        with st.expander(f"üåê External Links ({len(external)} unique)"):
            st.markdown("*These links point to external websites*")

            # Sort by count descending - handle both old and new format
            try:
                sorted_links = sorted(external.items(), key=lambda x: x[1]['count'] if isinstance(x[1], dict) else x[1], reverse=True)
            except (KeyError, TypeError):
                st.error("‚ö†Ô∏è Link data format mismatch. Please re-run the extraction.")
                return

            for url, data in sorted_links[:30]:  # Show top 30
                # Handle both old format (int) and new format (dict)
                if isinstance(data, dict):
                    with st.container():
                        col1, col2 = st.columns([5, 1])
                        with col1:
                            # Show anchor text(s)
                            anchor_texts = data.get('texts', ['No text'])[:3]
                            st.markdown(f"**{anchor_texts[0]}**" + (f" *(+{len(data['texts'])-1} more)*" if len(data.get('texts', [])) > 1 else ""))
                            st.caption(url if len(url) <= 80 else url[:80] + '...')

                            # Show source blog posts
                            sources = data.get('sources', [])[:3]
                            if sources:
                                source_text = ", ".join([f"[Post]({src})" for src in sources])
                                if len(data.get('sources', [])) > 3:
                                    source_text += f" *+{len(data.get('sources', []))-3} more*"
                                st.markdown(f"Found in: {source_text}", unsafe_allow_html=True)
                        with col2:
                            st.metric("Uses", data.get('count', 0))
                        st.markdown("---")
                else:
                    # Old format: just show count
                    st.text(f"{url[:80]}... (√ó{data})")
                    st.markdown("---")

            if len(sorted_links) > 30:
                st.info(f"... and {len(sorted_links) - 30} more external links")

def display_find_replace():
    """Simple find/replace interface for link modification"""
    if not st.session_state.get('link_analysis'):
        return

    internal = st.session_state.link_analysis.get('internal', {})
    if not internal:
        return

    st.markdown("### üîÑ Fix Internal Links")
    st.write("Change domain names before downloading (useful when migrating blogs)")

    # Initialize replacements in session state
    if 'replacements' not in st.session_state:
        st.session_state.replacements = []

    # Simple two-field form
    col1, col2 = st.columns(2)
    with col1:
        find_text = st.text_input("Find:", placeholder="blog.oldsite.com")
    with col2:
        replace_text = st.text_input("Replace with:", placeholder="newsite.com/blog")

    if find_text and replace_text:
        if st.button("Add Replacement"):
            st.session_state.replacements.append((find_text, replace_text))
            st.success(f"‚úÖ Will replace '{find_text}' with '{replace_text}'")
            st.rerun()

    # Show active replacements
    if st.session_state.replacements:
        st.write("**Active replacements:**")
        for i, (find, replace) in enumerate(st.session_state.replacements):
            col1, col2 = st.columns([4, 1])
            with col1:
                st.write(f"‚Ä¢ {find} ‚Üí {replace}")
            with col2:
                if st.button("‚ùå", key=f"remove_{i}"):
                    st.session_state.replacements.pop(i)
                    st.rerun()

        if st.button("Clear All Replacements"):
            st.session_state.replacements = []
            st.rerun()

def apply_replacements(xml_content: str) -> str:
    """Apply all replacements to XML content"""
    if not st.session_state.get('replacements'):
        return xml_content

    modified_content = xml_content
    for find_text, replace_text in st.session_state.replacements:
        modified_content = modified_content.replace(find_text, replace_text)

    return modified_content

def process_urls(urls: List[str], max_concurrent: int = 1):
    """Process URLs with progress tracking (supports async concurrent mode)"""
    if not urls:
        st.error("‚ùå No valid URLs to process")
        return

    # Initialize progress tracking
    progress_bar = st.progress(0)
    status_text = st.empty()
    results_container = st.empty()
    log_container = st.empty()

    # Create callback for logging
    def logging_callback(level: str, message: str):
        """Callback to display logs in Streamlit - filters verbose messages"""
        # Filter out verbose internal messages for cleaner UI
        skip_phrases = [
            "Fetching with",
            "attempt",
            "Retrying in",
            "Detected platform",
            "Platform:",
            "All",
            "failed",
        ]

        # Skip verbose messages unless it's an error
        if level != 'error' and any(phrase in message for phrase in skip_phrases):
            return

        with log_container.container():
            if level == 'error':
                st.error(f"üî¥ {message}")
            elif level == 'warning':
                st.warning(f"‚ö†Ô∏è {message}")
            else:
                st.info(f"‚ÑπÔ∏è {message}")

    # Initialize extractor with callback
    extractor = BlogExtractor(callback=logging_callback, verbose=False)

    # Reset session state
    st.session_state.extraction_results = []
    st.session_state.error_log = []
    st.session_state.duplicate_log = []

    start_time = time.time()

    # Use concurrent processing if enabled
    if max_concurrent > 1:
        status_text.text(f"‚ö° Processing {len(urls)} URLs with {max_concurrent} concurrent requests...")

        # Run async processing
        results = asyncio.run(extractor.process_urls_concurrently(urls, max_concurrent))

        # Process results
        for i, result in enumerate(results):
            progress = (i + 1) / len(results)
            progress_bar.progress(progress)

            if result and result.get('status') == 'success':
                st.session_state.extraction_results.append(result)
                with results_container.container():
                    title = result.get('title', 'No title')[:60] + ('...' if len(result.get('title', '')) > 60 else '')
                    st.success(f"‚úÖ **{title}**")

            elif result and result.get('status') == 'duplicate':
                st.session_state.duplicate_log.append({
                    'url': result.get('url', ''),
                    'title': result.get('title', 'Unknown')
                })
                with results_container.container():
                    st.warning(f"‚äò Duplicate: {result.get('title', 'Unknown')}")

            else:
                st.session_state.error_log.append({
                    'url': result.get('url', ''),
                    'error': result.get('error', 'Unknown error')
                })
                with results_container.container():
                    st.error(f"‚ùå Failed: {result.get('error', 'Unknown error')}")

        elapsed = time.time() - start_time
        status_text.text(f"‚úÖ Completed in {elapsed:.1f} seconds!")
        progress_bar.progress(1.0)

    # Sequential processing (original behavior)
    else:
        for i, url in enumerate(urls):
            # Update progress
            progress = (i + 1) / len(urls)
            progress_bar.progress(progress, text=f"Processing {i + 1}/{len(urls)} URLs")
            status_text.text(f"üîÑ Processing: {url}")

            try:
                # Extract blog data
                result = extractor.extract_blog_data(url)

                if result and result.get('status') == 'success':
                    st.session_state.extraction_results.append(result)

                    # Display detailed success info
                    title = result.get('title', 'No title')[:60] + ('...' if len(result.get('title', '')) > 60 else '')
                    content_length = len(result.get('content', ''))
                    links_count = len(result.get('links', []))
                    categories = ', '.join(result.get('categories', []))

                    success_msg = f"‚úÖ **{title}**"
                    details = f"Content: {content_length:,} chars | Links: {links_count} | Categories: {categories or 'None'}"

                    with results_container.container():
                        st.success(success_msg)
                        st.text(f"   {details}")
                        st.text(f"   URL: {url}")
                        st.write("---")

                elif result and result.get('status') == 'duplicate':
                    st.session_state.duplicate_log.append({
                        'url': url,
                        'title': result.get('title', 'Unknown')
                    })
                    with results_container.container():
                        st.warning(f"‚äò Duplicate: {result.get('title', 'Unknown')}")
                        st.text(f"   URL: {url}")
                        st.write("---")

                else:
                    st.session_state.error_log.append({
                        'url': url,
                        'error': result.get('error', 'No content extracted')
                    })
                    with results_container.container():
                        st.error(f"‚ùå Failed - {result.get('error', 'No content extracted')}")
                        st.text(f"   URL: {url}")
                        st.write("---")

            except Exception as e:
                st.session_state.error_log.append({
                    'url': url,
                    'error': str(e)
                })
                with results_container.container():
                    st.error(f"‚ùå Failed - {str(e)}")
                    st.text(f"   URL: {url}")
                    st.write("---")

            # Add small delay to make progress visible
            time.sleep(0.5)

        # Processing complete
        elapsed_time = time.time() - start_time
        progress_bar.progress(1.0)
        status_text.text(f"‚úÖ Processing complete! ({elapsed_time:.1f} seconds)")

    # Update session state
    st.session_state.is_processing = False
    st.session_state.extraction_complete = True

    # Analyze links for internal/external categorization
    if st.session_state.extraction_results:
        link_analysis = analyze_links(st.session_state.extraction_results)
        st.session_state.link_analysis = link_analysis

    # Generate output files
    generate_output_files(extractor)

def generate_output_files(extractor: BlogExtractor):
    """Generate WordPress XML and links content - no files created"""
    try:
        # Set extracted data in extractor
        extractor.extracted_data = st.session_state.extraction_results

        # Generate XML content
        st.session_state.xml_content = extractor.get_xml_content()

        # Generate links content
        st.session_state.links_content = extractor.get_links_content()

    except Exception as e:
        st.error(f"‚ùå Error generating output content: {e}")

def display_results():
    """Display extraction results and statistics"""
    if not st.session_state.extraction_complete:
        return

    st.markdown("### üìä Extraction Results")

    # Statistics
    success_count = len(st.session_state.extraction_results)
    duplicate_count = len(st.session_state.duplicate_log)
    failed_count = len(st.session_state.error_log)
    total_urls = success_count + duplicate_count + failed_count
    success_rate = (success_count / total_urls * 100) if total_urls > 0 else 0

    col1, col2, col3, col4, col5 = st.columns(5)
    with col1:
        st.metric("Total URLs", total_urls)
    with col2:
        st.metric("Successful", success_count)
    with col3:
        st.metric("Duplicates", duplicate_count)
    with col4:
        st.metric("Failed", failed_count)
    with col5:
        st.metric("Success Rate", f"{success_rate:.1f}%")

    # Results table with clickable blog post links
    if st.session_state.extraction_results:
        st.subheader("‚úÖ Successfully Extracted Posts")

        for idx, result in enumerate(st.session_state.extraction_results, 1):
            with st.container():
                col1, col2 = st.columns([5, 1])
                with col1:
                    # Clickable title linking to original blog post
                    title = result.get('title', 'N/A')
                    url = result.get('url', '')
                    st.markdown(f"**{idx}. [{title}]({url})**")

                    # Metadata
                    metadata_parts = []
                    if result.get('author'):
                        metadata_parts.append(f"üë§ {result['author']}")
                    if result.get('date'):
                        metadata_parts.append(f"üìÖ {result['date']}")
                    if result.get('platform'):
                        metadata_parts.append(f"üåê {result['platform'].title()}")

                    st.caption(" ‚Ä¢ ".join(metadata_parts))

                    # Categories and tags
                    info_parts = []
                    if result.get('categories'):
                        info_parts.append(f"üìÅ {', '.join(result['categories'][:3])}" + (f" +{len(result['categories'])-3}" if len(result['categories']) > 3 else ""))
                    if result.get('tags'):
                        info_parts.append(f"üè∑Ô∏è {', '.join(result['tags'][:3])}" + (f" +{len(result['tags'])-3}" if len(result['tags']) > 3 else ""))

                    if info_parts:
                        st.caption(" | ".join(info_parts))

                with col2:
                    st.metric("Content", f"{result.get('content_length', 0):,}")
                    st.metric("Links", len(result.get('links', [])))

                st.markdown("---")

    # Duplicate log
    if st.session_state.duplicate_log:
        st.subheader("‚äò Duplicate Content")
        with st.expander(f"View {len(st.session_state.duplicate_log)} duplicates"):
            for dup in st.session_state.duplicate_log:
                st.markdown(f"**[{dup['title']}]({dup['url']})**")
                st.caption(f"Skipped as duplicate content")
                st.markdown("---")

    # Error log
    if st.session_state.error_log:
        st.subheader("‚ùå Failed URLs")
        with st.expander(f"View {len(st.session_state.error_log)} failed URLs"):
            for error in st.session_state.error_log:
                st.markdown(f"**[{error['url']}]({error['url']})**")
                st.error(f"Error: {error['error']}")
                st.markdown("---")

def provide_downloads():
    """Provide download buttons for WordPress XML and links"""
    if not st.session_state.extraction_complete:
        return

    st.markdown("### üíæ Download Results")

    # Check if we have content
    if not st.session_state.get('xml_content'):
        st.info("No content generated yet. Please run the extraction first.")
        return

    # Two columns: XML and Links
    col1, col2 = st.columns(2)

    with col1:
        final_xml = apply_replacements(st.session_state.xml_content)
        label = "üìÑ WordPress XML"
        if st.session_state.get('replacements'):
            label += " (modified)"
        st.download_button(
            label=label,
            data=final_xml,
            file_name=f"blog_posts_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xml",
            mime="application/xml",
            help="WordPress XML file ready for import",
            use_container_width=True
        )

    with col2:
        if st.session_state.get('links_content'):
            st.download_button(
                label="üîó Extracted Links",
                data=st.session_state.links_content,
                file_name=f"extracted_links_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                mime="text/plain",
                help="All hyperlinks found in blog content",
                use_container_width=True
            )


def main():
    """Main application function"""
    # Initialize session state
    setup_session_state()

    # Display header
    display_header()

    # Get URLs and check if extract button was clicked
    urls, start_extraction = get_url_inputs()

    # Get concurrent settings (optional, collapsed by default)
    max_concurrent = get_concurrent_settings()

    # Start extraction if button was clicked
    if start_extraction and urls and not st.session_state.is_processing:
        st.session_state.is_processing = True
        st.session_state.extraction_complete = False

        st.markdown("---")
        st.markdown("### üîÑ Extracting Blog Posts...")

        with st.spinner("Processing your blog posts..."):
            process_urls(urls, max_concurrent)
    elif st.session_state.is_processing:
        st.warning("‚è≥ Extraction in progress...")

    # Display results
    display_results()

    # Display link analysis and find/replace interface
    display_link_analysis()
    display_find_replace()

    # Provide downloads
    provide_downloads()

if __name__ == "__main__":
    main()